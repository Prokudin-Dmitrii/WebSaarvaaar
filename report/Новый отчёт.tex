\documentclass[12pt, a4paper]{article}

\usepackage{graphicx}%Вставка картинок правильная
\graphicspath{ {images/} }

\usepackage{mathtools}  
\mathtoolsset{showonlyrefs}
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{array}
\definecolor{linkcolor}{HTML}{000080} % цвет ссылок
\definecolor{urlcolor}{HTML}{000080} % цвет гиперссылок
\hypersetup{pdfstartview=FitH,  linkcolor=linkcolor,urlcolor=urlcolor, colorlinks=true}
\usepackage{framed}
\definecolor{shadecolor}{RGB}{180,180,180}

\usepackage{float}%"Плавающие" картинки
\usepackage{subcaption}
\usepackage{wrapfig}%Обтекание фигур (таблиц, картинок и прочего)
\usepackage{amssymb,amsfonts,amsmath,cite,enumerate,float,indentfirst} %пакеты расширений
\usepackage{enumitem}
\makeatletter
    \AddEnumerateCounter{\asbuk}{\@asbuk}{ю)}
\makeatother

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}

\usepackage{geometry} % Меняем поля страницы
\geometry{left=1.5cm}% левое поле
\geometry{right=1.5cm}% правое поле
\geometry{top=1cm}% верхнее поле
\geometry{bottom=2cm}% нижнее поле
\newcommand{\Lagr}{\mathcal{L}}
\renewcommand{\theenumi}{\arabic{enumi}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumi}{\arabic{enumi}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\theenumii}{.\arabic{enumii}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}.}% Меняем везде перечисления на цифра.цифра
\renewcommand{\theenumiii}{.\arabic{enumiii}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumiii}{\arabic{enumi}.\arabic{enumii}.\arabic{enumiii}.}% Меняем везде перечисления на цифра.цифра

\begin{document}
\def\contentsname{Содержание}
\begin{titlepage}

\fontsize{15pt}{20pt}\selectfont
\begin{center}
\fontsize{15pt}{20pt}\selectfont
Московский государственный университет имени М.В. Ломоносова
\end{center}
\begin{figure}[H]
    	\centering
    \includegraphics[scale = 1.3]{MSU_IM}
   % \includegraphics[width=0.25\textwidth]{mesh}
    %\caption{a nice plot}
  %  \label{fig:mesh1}
\end{figure}

\begin{center}
\fontsize{15pt}{20pt}\selectfont
Факультет вычислительной математики и кибернетики

Кафедра Математических Методов Прогнозирования
\end{center}
\vspace{10em}

\begin{center}
\begin{bfseries}
\fontsize{20pt}{25pt}\selectfont
\linespread{2}
Отчёт о выполнении задания №3:\\"Ансамбли алгоритмов.

Композиции алгоритмов для решения задачи регрессии"\\
\fontsize{15pt}{20pt}\selectfont
по курсу "Практикум на ЭВМ"

\end{bfseries}
\end{center}
\vspace{5em}
\begin{flushright}
\fontsize{15pt}{20pt}\selectfont
Прокудин Дмитрий\\
Сергеевич\\
317 группа
\end{flushright}
\vspace{\fill}
\begin{center}
\fontsize{15pt}{20pt}\selectfont
Москва\\2022
\end{center}

\end{titlepage}

\newpage
\setcounter{page}{2}
\tableofcontents{}
\addtocontents{toc}{~\hfill\textbf{Страница}\par}

\newpage
\section{Введение}
В данном задании осуществляется детальное ознакомление с алгоритмами композиций: случайным лесом и градиентным бустингом - проводится исследование зависимости точности моделей и времени обучения от их сложности на примере задачи регрессии с использованием датасета данных о продаже недвижимости. Для выполнения задания на языке Python пишутся собственные реализации случайного леса и градиентного бустинга на основе \colorbox{gray!20}{\textbf{DecisionTreeRegressor}} из библиотеки scikit-learn.

\section{Эксперименты}
\subsection{Предобработка данных}

Эксперименты этого задания проводятся датасете данных о продаже недвижимости "House Sales in King County, USA". Пример датасета представлен в таблице \ref{Ex_1}. Всего в датасете 21 признак и  $21613$ записи.


\begin{table}[H]
\caption{Пример первых 7 столбцов изначального датасета}
\begin{center}
\begin{tabular}{lrlrrrrr}
\toprule
{} &          id &             date &     price &  bedrooms &  bathrooms &  sqft\_living &  sqft\_lot \\
\midrule
0 &  7129300520 &  20141013T000000 &  221900.0 &         3 &       1.00 &         1180 &      5650 \\
1 &  6414100192 &  20141209T000000 &  538000.0 &         3 &       2.25 &         2570 &      7242 \\
2 &  5631500400 &  20150225T000000 &  180000.0 &         2 &       1.00 &          770 &     10000 \\
\bottomrule
\label{Ex_1}
\end{tabular}
\end{center}
\end{table}


Для дальнейшей работы с датасетом из него были извлечены три столбца: 'price'  - как целевая переменная, 'id' и 'date'  - как столбцы, имеющие не скалярный тип. Датасет из оставшихся $18$ столбцов без дополнительных изменений был преобразован в формат  numpy.ndarray, а затем разделён на обучающую и валидационную/отложенную выборки в отношении $7:3$.

\subsection{Исследование точности и времени работы алгоритмов}

Для изучения зависимости точности алгоритмов, а также времени их работы от сложности модели проводится перебор следующих гиперпараметров моделей:
\begin{itemize}
\item Количество деревьев в ансамбле: $1 - 900$;
\item Размерность подвыборки признаков для одного дерева: $[1, 6, 12, 18]$ - для случайного леса и $[6, 12, 18]$ - для градиентного бустинга;
\item Максимальная глубина дерева: $[1, 5, 10, 15,$ без\_ограничения$]$ - для случайного леса и $[1, 5, 15,$ без\_ограничения$]$ - для градиентного бустинга;
\item Learning\_rate: $[0.1, 0.5, 1.0, 2]$ - только для градиентного бустинга.
\end{itemize}

Зависимость точности (на метрике RMSE) для случайного леса представлена на рис.\ref{RMSERF}. Далее на графиках количество деревьев в ансамбле отмечено по оси x; в легенде 'fss' обозначает размер подвыборки признаков для каждого дерева, а 'm\_d' - максимальную глубину каждого дерева (для градиентного бустинга параметр 'lr' отвечает за learning\_rate).


Зависимость времени обучения для случайного леса представлена на рис.\ref{TimeRF}. Замеры времени проводились при добавлении новых $100$ деревьев в модель, то есть график построен по $10$ точкам (аналогичным образом замеры проводились и для градиентного бустинга). 

\begin{figure}[H]
\centering
\includegraphics[scale = 0.56]{RMSE_RF_2}
\caption{Зависимость значений RMSE от параметров случайного леса.}
\label{RMSERF}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[scale = 0.56]{ВремяRF}
\caption{Зависимость времени обучения от параметров случайного леса.}
\label{TimeRF}
\end{figure}

На основе результатов экспериментов для случайного леса можно сделать следующие выводы:
\begin{itemize}
\item Время обучения ансамбля линейно зависит от количества деревьев, при этом время обучения каждого дерева растёт при увеличении размера подвыборки признаков и глубины деревьев.
\item Результаты ансамблей из деревьев малой глубины ($1, 5$) существенно хуже результатов ансамблей с большей глубиной (значение RMSE больше в $1.5 - 2$ раза). Самые точные ансамбли построены с использованием деревьев без ограничения на глубину.
\item Размер подвыборки признаков для каждого дерева оказывает значительно меньшее влияние на итоговую точность, чем глубина деревьев.
\item При большом количестве деревьев в ансамбле добавление новых деревьев не оказывает существенный вклад на точность.
\end{itemize}

Зависимость точности (на метрике RMSE) для градиентного бустинга представлена на рис.\ref{RMSE11GB}, \ref{RMSE12GB}. Большое значение learning\_rate $= 2$ привело к тому, что алгоритмы не сошлись, а точность упала, поэтому на рис.\ref{RMSE2GB} представлены результаты измерений точности для learning\_rate $= 0.1$ (результаты для остальных значений learning\_rate имеют аналогичный вид и не представлены на графиках). 


Зависимость времени обучения для градиентного бустинга представлена на рис.\ref{TIME1GB} и рис.\ref{TIME2GB}.


\begin{figure}[H]
\centering
\includegraphics[scale = 0.56]{RMSE11GB}
\caption{Зависимость значений RMSE от параметров градиентного бустинга (первая половина параметров).}
\label{RMSE11GB}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale = 0.56]{RMSE12GB}
\caption{Зависимость значений RMSE от параметров градиентного бустинга (вторая половина параметров).}
\label{RMSE12GB}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale = 0.56]{RMSE22GB}
\caption{Зависимость значений RMSE от параметров градиентного бустинга (для lr = 0.1).}
\label{RMSE2GB}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale = 0.56]{Время1GB}
\caption{Зависимость  времени обучени от параметров градиентного бустинга (первая половина параметров).}
\label{TIME1GB}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale = 0.56]{Время2GB}
\caption{Зависимость  времени обучени от параметров градиентного бустинга (вторая половина параметров).}
\label{TIME2GB}
\end{figure}

На основе результатов экспериментов для градиентного бустинга можно сделать следующие выводы:
\begin{itemize}
\item Для большинства комбинаций параметров время обучения зависело от количества деревьев линейно, однако есть примеры, где данная зависимость не наблюдается. При этом время обучения моделей градиентного бустинга оказалось больше времени обучения моделей случайного леса.
\item Точность ансамблей зависит от глубины деревьев и размера подвыборки признаков в меньшей степени. Кривая точности имеет достаточно гладкий вид. 
\item Большое значение learning\_rate ( $> 1$) привело к 'расхождению' модели.
\item Аналогично случайному лесу при большом количестве деревьев в ансамбле добавление новых деревьев не оказывает существенный вклад на точность.
\end{itemize}
\section{Вывод}
\begin{itemize}
\item В ходе выполнения данного задания было исследовано поведение точности и времени обучения ансамблей решающих деревьев в зависимости от сложности модели: время обучения и точность растут с увеличением глубины деревьев и размера подвыборки признаков.
\item Градиентный бустинг показал результат лучше, чем случайный лес, но незначительно дольше обучался. 
\item Несмотря на небольшие различия в точности, обе модели хорошо подходят для решения данной задачи, время обучения отдельной модели невелико. 
\item Влияние гиперпараметров (количества деревьев, глубины и размера подвыборки признаков) на точность модели у случайного леса значительнее, чем у градиентного бустинга.
\end{itemize}

\end{document}
